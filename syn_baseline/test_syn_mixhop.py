import os
import random
import collections
import numpy as np
import torch
import pickle
import argparse
import scipy.sparse as sp
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torch_geometric.utils import to_scipy_sparse_matrix
from tqdm import trange
import copy
import time
from data_loader import DataLoader, ResultRecorder
from GCN import GCN, GAT, GAT2, MLP
from sklearn.metrics import f1_score
import dgl
from torch_geometric.utils import to_dense_adj
import sys
sys.path.append("..")
from utils.helper import get_eig
from dgl import ops


# syn data parameters
parser = argparse.ArgumentParser(description="Test synthetic graph dataset generated by Mixhop")
parser.add_argument("--h", type=float, default=0.0, help="number of nodes per class in the synthetic dataset",)
# model training parameters
parser.add_argument('--cuda', type=int, default=0, help='Avaiable GPU ID')
parser.add_argument('--method', type=str, default='GAT', help='which model to')
parser.add_argument('--run', type=int, default=5, help='number of splits per graph')
parser.add_argument('--epoch_num', type=int, default=1000, help='Number of Epoch')
parser.add_argument('--batch_size', type=int, default=99999, help='size of output node in a batch')
parser.add_argument('--dropout', type=float, default=0.1, help='Dropout rate')
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--weight_decay', type=float, default=1e-3)
args = parser.parse_args()

# generate data & splits and put them in the corresponding folder
BASE_DIR = "./syn_mixhop/"
SPLIT_DIR = "./syn_split/"
device = torch.device("cuda:" + str(args.cuda))

def edge_mixhop_to_edge_list(edge_mixhop):
    adj_indices = []
    for node, neighbors in edge_mixhop.items():
        for n in neighbors:
            adj_indices.append([node, n])
    return np.transpose(adj_indices)

feat = np.load(os.path.join(BASE_DIR, "ind.n5000-h%.1f-c10.allx" % (args.h)))
label = np.load(os.path.join(BASE_DIR, "ind.n5000-h%.1f-c10.ally" % (args.h))) # one-hot label
edge_mixhop = pickle.load(
    open(os.path.join(BASE_DIR, "ind.n5000-h%.1f-c10.graph" % (args.h)), 'rb'), 
    encoding='latin1')
num_nodes = len(edge_mixhop)
edge_list = edge_mixhop_to_edge_list(edge_mixhop)
edge_list = torch.tensor(edge_list).to(device)
adj = to_scipy_sparse_matrix(edge_list)
label = label.argmax(1)

# remove the first isolated node
adj = adj.toarray()
adj = adj[1:, 1:]
label = label[1:]
feat = feat[1:, ]

num_classes = label.max() + 1
labels_th = torch.LongTensor(label).to(device)
feat_data_th = torch.FloatTensor(feat).to(device)
criterion = nn.CrossEntropyLoss()

adj = sp.csr_matrix(adj)

acc_list = []; train_time = []
exec(open("./gcn_helper.py").read())

torch.manual_seed(0)

def remove_node0(nodes):
    # remove the node zero, return new index
    nodes = nodes - 1
    if -1 in nodes:
        nodes = np.delete(nodes, -1)
    return nodes

for split_idx in range(args.run):
    # prepare data
    split = torch.load(os.path.join(SPLIT_DIR, f"ind.n5000-h{args.h}-c10_split_{split_idx}.pt"))
    train_nodes, valid_nodes, test_nodes = split['train'], split['val'], split['test']
    train_nodes = remove_node0(train_nodes)
    valid_nodes = remove_node0(valid_nodes)
    test_nodes = remove_node0(test_nodes)
    data_loader = DataLoader(adj, train_nodes, valid_nodes, test_nodes, device)
    #select model
    if (args.method == "GCN"):
        model = GCN(n_feat=feat_data_th.shape[1], n_hid=64, n_layers = 2,
                    n_classes=num_classes, dropout=args.dropout, criterion=criterion).to(device)
    elif (args.method == "GCN1"):
        model = GCN(n_feat=feat_data_th.shape[1], n_hid=64, n_layers = 1,
                    n_classes=num_classes, dropout=args.dropout, criterion=criterion).to(device)
    elif (args.method == "GAT"):
        model = GAT(n_feat=feat_data_th.shape[1], n_hid=8,
                    n_classes=num_classes, dropout=args.dropout, criterion=criterion).to(device)
    elif (args.method == "GAT2"):
        model = GAT2(n_feat=feat_data_th.shape[1], n_hid=8,
                    n_classes=num_classes, dropout=args.dropout, criterion=criterion).to(device) 
    elif (args.method == "MLP"):
        model = MLP(n_feat=feat_data_th.shape[1], n_hid=64, 
                    n_classes=num_classes, dropout=args.dropout, criterion=criterion).to(device)
    print(model)
    # train     
    results, acc, avg_time = train(
        model, data_loader, device, note="%s (layers = 2)"%(args.method)
    )
    eval_model = results.best_model.to(device)
    output = eval_model(feat_data_th, data_loader.lap_tensor)
    output = output.argmax(dim=1)        
    acc = f1_score(
        output[test_nodes].detach().cpu(), 
        labels_th[test_nodes].detach().cpu(), 
        average="micro")
    acc_list.append(acc)
    train_time.append(avg_time)

test_mean = np.mean(acc_list)
test_std = np.std(acc_list)
filename = f'./syn_results/syn_results_dropout0.1.csv'
print(f"Saving results to {filename}")
with open(f"{filename}", 'a+') as write_obj:
    write_obj.write(f"{args.method.lower()}, " +
                    f"{args.h}, " +
                    f"{test_mean:.4f}, " +
                    f"{test_std:.4f}, " +
                    f"{args}\n")